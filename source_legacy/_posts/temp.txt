Awesomeâ€”letâ€™s walk the whole ReFSQL pipeline end-to-end, with what happens at **training** vs **inference**, and where each equation/section lives in the paper.

---

Great question ğŸ‘ â€” they often look similar in form, but the **intent and setup** are different. Letâ€™s break it down:

---

### ğŸ”¹ **Cross-Entropy (CE) Loss**

* **Use case**: Supervised learning (classification, sequence prediction).
* **Setup**: You have a model outputting a probability distribution over classes (via softmax). You also have a *ground-truth label* (one-hot).
* **Objective**: Minimize the negative log-likelihood of the true label.

$$
L_{CE} = - \log \frac{\exp(s_{y})}{\sum_{j}\exp(s_{j})}
$$

where $s_j$ are the logits and $y$ is the correct class.

*â€œPush probability mass onto the correct label.â€*

---

### ğŸ”¹ **InfoNCE Loss**

* **Use case**: Contrastive/self-supervised learning (representation learning).
* **Setup**: You donâ€™t have labels. Instead, you define:

  * **Anchor** representation $h_i$.
  * **Positive sample** $h_i^+$ (a â€œtrueâ€ match, e.g., augmented view of the same image, or structurally similar SQL).
  * **Negative samples** $\{h_i^-\}$ (other data points in the batch).
* **Objective**: Maximize similarity with positives, minimize similarity with negatives.

$$
L_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(h_i, h_i^+)/\tau)}{\sum_{j}\exp(\text{sim}(h_i, h_j)/\tau)}
$$

where $\tau$ is a temperature.

*â€œAmong all candidates, treat the positive as the correct class and all others as negatives.â€*
