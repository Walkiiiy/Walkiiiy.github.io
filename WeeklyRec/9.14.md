# 9.8
- base model和new model必须有相同的输入

- 不能同时引入一个新benchmark的同时提出一个新的model？？？

- 疑点：
  - 要用同样的训练材料训练bench model才能证明新的model方法有效？即使训练材料和和测试材料不属于同一个数据集也不行？？本来以为只要输入的prompt以及外部知识文档相同就行？？？？
  - 创新点到底是啥啊
  - **rules能不能算外部知识**，还是只能用原本的“evidence”作为外部知识，或许是将rules的通用性增强？
  - 将所有rules划分成训练集测试集而不是一并作为输入文档，有没有表示清楚
  - 传统text2sql问题原因总结：
    - schema格式决定的错误
    - 问题指代不清具体的表和列
    - 输出的格式，风格不匹配，正确的思路也会判错
    - 缺少额外的知识：公式，背景等

- 实际上数据库的“外部知识”在研究中并没有明确完整的定义，规则库与外部知识的界限是模糊的（来自gpt

- 关键在于，原本的document（evidence）中根本就不包含错误点，sql出错不是因为没有正确理解evidence，而是因为上面总结的四点原因，

- 最坏的情况，假如rules不能作为输入，那能不能：
    - 只能使用bird自带的rules输入（evidence）
    - 合成的全部rules作为训练材料
    - 用合成rules作为材料训练assmuer：提取intention，keyword，operation
      - when <intention><keyword>,when <intention><keyword>:,when <intention><keyword>:<operation>
      - similarty=k1*sim(intention)+k2*sim(keyword)
      - retreieve用
- 或者还有一种思路：
  - 原本的bird-evidence作为测试集，spider作为训练集，以上的assumer-agent训练
  - 在此基础上还可以附加合成的evidence到spider，或者混杂合成evidence-测试
- 188/185 3090  qwen3b
- multi agent

# 9.9
- 先试试retreieve
- 流程：
  - 对原本的bench进行实验A：自带的evidence作为外部知识document，在prompt输入
  - Spier_train，Bird_train 分为两部分，训练和验证assumer,parser,planner（微调材料）
  - 对原本的bench进行实验B
  - Spider_dev Bird_dev的合成document用于测试（在prompt中输入）
- recoverAPP 框架搭建 
  - 视频选择
  - 进入界面播放视频
  - 播放完毕弹出分数结果
# 9.10
- 补充流程图
# 9.11
- 痛点-解决方法:
  - 连接内外部知识（示例sql）的可解释性缺失：rules
  - LLM倾向自然语言，传统方法缺少对示例sql的理解分析和自然语言规律抽取（以往研究直接将相似sql输入prompt，）： amend-rule extraction
  - 以往text2sql机制的问题：输出格式不匹配，schema信息重点不突出，：rules的局部性和相似性 assumer的相似性推理
  - 未实现）传统偏好生成机制在生成内容时刻意避开关键词或使用反义词，缺少局部创造性：？assumer的创造性推理
  - 规则抽取-创造型发散的mutiagent text2sql框架

- retreieve 写完
  - 问题：相似的condition有不相似的operation，本质问题是控制创新性与相似性
  - 方案1: 选取所有condition和operation相似读都高于阈值的材料训练assumer
    - 否决，不能产生两套，应该融合调整创造与仿造的比例：仿写-创造 token ，并行生成similar和creativity驱动的rules，creativity的损失函数可以调低，奖励调高
    - condition similar, operation similar: do similarity driving assume
    - consition not similar, operation similar: do creativity driving assume
    - n different types of assume, select one do assume (speciall token)
    - train two models for each method, when practicing, embedding the similar method into creativity method
    - 将所有的operation用关键词，文法进行分类，每种类型取最高相似度condition-operation pair， 
  - 方案2：放弃生成一模一样的rules
    - 损失函数全部用相似度
    - 增加输入的top similar-rules数量（similar根据condition定）
    - 训练assumer时只要求相似
    - 训练parser和planner时额外需要让其验证assumer生成的rules是否适合（viable/relevant/）如果判定为不相关，则在接下来的生成中尽量远离similar-rules（的种类）并调高创新性。具体怎么“远离”而不是偏向反义词，需要讨论。如果相关，直接拿来用
    - 或者直接把上部作为assumer自己的工作，在不用模型生成相似rules，直接retreieve过来相似rules，然后在assumer的prompt中加入完整的question-schema-analyze， planner/parser来负责判定相关性在相关性低的时候调用assumer产生创新rules，并在verifier验证后返回给planner/parser。

- 远离而非反义内容生成：generating random but similar tokens based on the given context, then verification.


# 9.12 9.13
- assumer整体训练方法：对于高低与阈值的conditon和operation分别怎么办，还要有一个base
- needs training: 
  - parser: 
    - combining metadata, translating the given user question to Accurate, Elaborate question(model question)
    - generating the <Doc Retreieve> token when meeting ambiguous.
    - use <Positive><Negative> token judge if the similar-conditioned rules are useful.
    - call tools <Assumer> <Verifer> <FetchSchema> 
  - planner:
    - transform the detailed question to detailed NL implement steps
    - generating the <Doc Retreieve> token when meeting ambiguous.
    - use <Positive><Negative> token judge if the similar-conditioned rules are useful.
    - call tools <Assumer> <Verifer> <FetchSchema> 
  - assumer:
    - generate *"structral novel yet content similar"* rules, pass the novel rules back to planner
  - verifer:
    - ......
- 推理时： 
  - planner/parser detect an possible ambiguousity, generate a <DOC Retreieve> token, retreieving similar rules by condition similarity, 
  - then combining the schema and question, infer if the retreieved rules are useful or not.
  - if useful, generate <Positive> token, conclude rules.
  - if not useful, generate <Negative> token, call assumer, pass the schema-question-NegRules to assumer
  - assumer generate *"structral novel yet content similar"* rules, pass the novel rules back to planner
  - planner call verifer to verify if the rules (Possitive or Novel ones) are viable.
- 高condition高operation：
- 远离而非反义内容生成具体措施

# 9.14
- 横向演示
- 
- rules自动化提取流程演示

- 不要优化一个不该存在的方案
- 
- 原本的计划是提出一个带有rules的benchmark，并在这个标准上提出自己的模型，证明自然语言规则在相似sql之间的通用性和可解释性，挖掘针对特定风格数据库的sql自然语言规则的潜力。但同时提出bench和模型是不可取的，因此重点放在模型本身，把原本跑出来的bench大部分数据用来训练模型。

- text2sql模型可解释性规则：NL rules generation和agentic parser-planner structure
- 针对同一schema的question，相似性规则利用：Assumer
- 分析目前的text2sql错误原因（四种），两种规则解决
  - question风格与通用背景知识挖掘缺失：global Assumer defination rules（A means B）
  - sql select格式与schema重点提取：local Assumer condition rules(when....do.....)

- 题目模糊点：模型输入,原本的方案中模型输入包含正误sql examples，意味着原始的benchmark在被测试时输入的数据量比标准要少（因为需要提取每一个数据库的example用于生成该数据库的rules），方法的输入可能不标准（修改了benchmark）
- 假如原始方案（输入正误sql对）不用，替代方案：训练assumer**只用schema信息推理出可能的rules**，parser和planner根据不同的方案retreieve.
- 实验模糊点：模型规模与实际效果SOTA对比




## 9.15
- 方案二有类似的从schema中挖掘NL evidence有人做过

- 创新点总结：

  - 现有模型用的外部知识(evidence)是针对question的，缺少通用性，没有有效利用同一schema下的语义和结果相似性：
    - 提出新的外部知识结构：rules=evidence / output style / schema weakness
    - 训练assumer在同一 schema 范围内挖掘rules。利用规则假设assumption rules，提升模型对同一 schema 下新问题的泛化能力
  
  - 现有 Text-to-SQL 模型缺乏透明性，难以解释推理过程:
    - NL Rules Generation + Parser-Planner Structure 将“解析器”与“计划器”解耦，形成可解释的中间步骤，并由assumer为每一步可能的模糊错误点生成rules

  - 现有 Text-to-SQL 模型通常采用端到端生成，容易在复杂 SQL 推理中出现遗漏或错误，准确率受限：
    - （问题拆解成workflow提高ac
    - Multi-Agentic Parser-Planner Structure，将 SQL 生成过程拆解为多个 agent（解析、规划、生成、验证），实现模块化协作

- 顶会论文不能将api调用作为模型结构的一部分

- 找到合适的模型做可行性验证  
