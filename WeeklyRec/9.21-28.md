## 9.18
- arctic prompt对齐不完全，schema的问题，acc还是不能复线
- 讨论：蒸馏deepseek/微调qwen3 （可考虑复现Arctic?）做generator，效果实在不行再调api
- 不行中间部分用复现的bird开源模型

## 9.19
- qwen3 14b基准测试
- qwen3 8b基准测试
- 分析Arctic：为什么跑不通
- qwen3 8b generator微调
- 略读sigmod text2sql文献：不一定提出复杂method
- 创新点不够：多阅读论文，结构和方法可以改
- 并行：调模型，优化创新性
- 实验:不仅要设计rules对比，还要设计加chain和没有chain的对比
- 应该是从后往前训练：
  - 生成可用的generator训练材料：自然语言步骤。微调generator生成目标sql能力
  - 用自然语言步骤反推完整的问题解析，生成可用的parser训练材料，微调训练parser生成问题解析能力
  - 用反推得到的材料（问题解析和自然语言步骤）微调planner生成自然语言步骤能力
  - 整体测试
- 生成可以考虑用rules-extraction类似方法，多轮比对提取-改进，确保生成正确内容




## 9.20
- sql2plan prompt:
  ```
  You are a precise translator that converts an SQL query into a step-by-step natural-language execution plan.  
  Your job: given one SQL query, output a numbered list of steps describing exactly how to execute that query so someone (or another program) can reconstruct the SQL from the description. Accuracy is the top priority. The plan **must not use SQL keywords** (for example: select, from, where, join, group, having, order, limit, distinct, union, intersect, except, exists, in, subquery, alias, as) in the plan itself. Use only ordinary English and precise descriptions.

  Strict rules:
  1. Output only the numbered plan (one step per line). Do not include commentary, extraneous explanation, or the original SQL. If the SQL is invalid, output a single numbered step that clearly states the error in plain language.
  2. Preserve table and column names exactly as they appear in the SQL. When referencing them inside steps, put each name in square brackets like [table] or [table.column] so names remain unambiguous.
  3. If the query creates intermediate result sets (derived tables / nested queries), label them explicitly as "intermediate set 1", "intermediate set 2", etc., and describe exactly how each intermediate set is produced and what columns it contains.
  4. For filters, use exact logical conditions in plain English, e.g. "keep rows where [age] is greater than 30" or "keep rows where [status] is 'active' and [score] >= 80".
  5. For combining rows from multiple tables, describe the combination as: "combine rows from [A] and [B] where [A.col] equals [B.col]" and explicitly state the semantics:
    - If only matching pairs are kept, say: "keep only combinations that have a match in both sides."
    - If all rows from the left side are kept even when there is no match, say: "keep all rows from [left table]; when there is no matching row on the right, leave right-side columns empty."
    - If all rows from the right side are kept, describe symmetrically.
  6. For aggregation, avoid the phrase "group by". Instead say: "for each unique value of [column(s)] do the following: compute ..." and list aggregates precisely, e.g. "count of non-empty [id]" or "sum of [amount]" or "average of [duration]". If a filter applies to those aggregated results, say: "after computing those per-value results, keep only those groups where ...".
  7. For duplicate elimination, say: "remove duplicate rows so that values of [column list] are unique".
  8. For ordering and limiting, say: "sort the final rows by [column] from smallest to largest (or largest to smallest). Then take the first N rows" — be explicit about whether sorting happens before or after limiting.
  9. For computed columns or expressions, write the formula in plain math/word form, e.g. "create a new column named 'ratio' equal to [col_a] divided by [col_b]".
  10. For boolean logic, express it with plain words ("and", "or", "not") and use parentheses language if needed: "apply both conditions (A and B)".
  11. For correlated checks that depend on each row (existence checks), describe them as: "for each row in [X], check whether there exists at least one row in [Y] such that ...; keep those rows where the check succeeds."
  12. For set operations (union / intersect / except), describe them in plain terms: "produce rows that are present in either set A or set B (without duplicates)" or "produce rows present in A that are not present in B", etc.
  13. For window functions, describe partitioning and ordering in plain English and the computed window metric, e.g. "for each partition defined by [col], compute the running total of [amount] ordered by [date]" and whether the metric is attached to every row or only used for filtering.
  14. If query uses naming shortcuts (aliases), show both the original name and the shortcut once, e.g. "[employees] (called E)" and thereafter you may refer to the shortcut in square brackets like [E.id], but still preserve the link to the original.

  Output format example (must follow this style exactly):

  1. Start with all rows from [people].
  2. Keep only rows where [age] is greater than 30.
  3. For each remaining row, take the values of [name] and [age].
  4. Sort those rows by [age] from largest to smallest.
  5. Return the first 5 rows.

  Do not use any SQL keywords inside the steps. Be concise but exact. Now convert the SQL query below into the required plan (replace the SQL delimiter with the actual SQL):

  -----SQL START-----
  <PUT THE SQL QUERY HERE>
  -----SQL END-----
  ```
- 先生成没有特殊token的plan做generator训练，然后加入tokens
- 之前的正确率低很有可能是因为LLMClient.chat没写好

## 9.21
- 合成数据集：sql2text qwen3 8b
- 如果要在训练方法上有创新，创新点需要对text2sql任务有针对性
- 读读论文
  - Dail-SQL 优化数据集映射，使用优化后的训练集训练已有的模型，然后在优化后的测试集上测试，模型能力提升
  - 
- 方案1:自训练
- 方案2:GRPO
- 方案3:调api
- 或者改方案



- 修改结构
- 是不是不能拿Arctic来改进？要构建自己的ppe

- 把parser-planner-excute合并成一个模型具体训练方法仿照self-rag
  - 具体方案：
  - 是否能考虑使用api合成少量训练数据
  - 分为三个模型的训练量太大，而且对于小模型来说把计划和执行分开反而会降低成功率，事实上知识简单的sql转plan再转回sql，Qwen3 8B在Bird上只有40不到的成功率。

## 9.22
- 之前提出的架构太大太分散，导致难以实际训练部署，无从下手
- **创新点是缺少在motivation还是解决方法？**
  
- **motivation** 在输入question足够明确的前提下，LLM大都能给出正确的查询思路。但现有text2sql仍存在问题：
  - 1.**无法理解question的隐含信息**。传统方法通过给question-sql pair加上一句扩展defination（definational external knowledge，也就是Bird中的evidence， A refers to B， C=...）来解决，但是**这极大削弱了实际应用时的鲁棒性**
  - 2.question理解正确，查询思路也正确，但**执行方式与预期不符**。常见的就是忽略/不考虑schema的特殊结构，以单一的经验化查询方式来写sql。传统的解决方法是通过隐式的operational external knowledge提取：
    - （1.schema-linking，通过向量相似度/minhash等算法，筛选相关性强的schema信息和具体值来输入，突出schema重点结构信息，仍需要模型自己挖掘发现重点信息
    - （2.RAG-SQL，从大量example文档中抽取相似程度高的quesiton-sql pair，加入prompt供模型学习。这种方法非常依赖相似性，缺少相似sql或者相似度比较算法都是其瓶颈
    - （3.比对学习、偏好学习，和方法2类似，抽取相关性强和相关性弱的question-sql对，来训练模型向风格相似的输出靠近，并远离结构相异的sql输出
    **而且上述方法都有一个共同的问题就是可解释性弱** 
  - 3. 思路和执行方式都正确，但**查询结果输出的方式和风格与预期不符**。在严格的测试环境或者挑剔的用户背景下，这样的输出全部判错。以往的研究多忽略这一点，重点训练模型根据question和schema制定查询策略，忽略其输出形式

- **contribution**
  - 提出text2sql的可解释NL rules，将definational和operational的external knowledge统一为自然语言规则
  - 提出基于相似性的可解释规则推理模型Rule Assumer（训练方法创新
  - （提出agentic规则验证模块Rule Verifier
  - 实验证明可解释的统一特征推理可以增强text2sql效果

- **Methodology**
  - 图
  - 四个模块：parse-plan-executor,definational_assumer,operational_assumer,verifier
  - 问题：最有说服力的框架和实验方式是什么

- **Training**
  - parser-planner-executor
    - 方案1分开训练
      - 自训练语料生成：从后往前依次推导：sql-->plan---（+ question+ sql）--->deteiled question
      - 每一步要保证生成的plan能被还原为sql，但是目前效果不佳
      - 可以简单改进，试试自训练+循环纠错效果，但是感觉意义不大
      - 后面还要分析哪些地方插入rules，用到了哪些rules，添加特殊token（self-rag方法）。大概率必须要api获取训练语料
    - 方案2
      - 三步当作整体一起训， 借鉴Arctic强化学习方法，难度大不保证效果
- **Experiment**




- rules分类：global rules 关键词： quesiton calculate refer answering
- global Assumer训练材料


## 9.23
- 测试golbalAssumer 一轮Spider三轮Bird 100条 bird_dev
  - 📊 FINAL RESULTS:
  With Rules:
    Correct Answers: 33
    Failed Executions: 0
    Accuracy: 0.330 (33.0%)
  Without Rules:
    Correct Answers: 37
    Failed Executions: 0
    Accuracy: 0.370 (37.0%)

  🎯 IMPROVEMENT:
      Rules provide -0.040 (-4.0%) accuracy improvement
      ❌ Rules hurt performance!
- 测试globalAssumer 一轮Spider三轮Bird 100条 bird_dev
  - 
  ```
  📊 FINAL RESULTS:
  With SFT Rules:
    Correct Answers: 33
    Failed Executions: 0
    Accuracy: 0.330 (33.0%)
  With Dataset Rules:
    Correct Answers: 55
    Failed Executions: 0
    Accuracy: 0.550 (55.0%)
  Without Rules (Baseline):
    Correct Answers: 33
    Failed Executions: 0
    Accuracy: 0.330 (33.0%)

  🎯 IMPROVEMENTS:
    SFT Rules vs No Rules: +0.000 (+0.0%)
    Dataset Rules vs No Rules: +0.220 (+22.0%)
    SFT Rules vs Dataset Rules: -0.220 (-22.0%)

  🏆 ANALYSIS:
      ➖ SFT rules have no effect on performance
      ✅ Dataset rules help improve performance over baseline!
      📚 Dataset rules perform better than SFT rules!      
      ```
- 测试中assumed rules出错而自带rules对的sample：
  ```
  Database: toxicology
Question: List all carcinogenic molecules and their elements.
Ground Truth: SELECT DISTINCT T2.molecule_id, T1.element FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule_id = T2.molecule_id WHERE T2.label = '+'

🤖 Step 1: Generating rules with fine-tuned model...
Generated Rules:
Rule 1:
Condition: When the question asks for elements of molecules
Operation: use "T1.element" from the "atom" table instead of "element" from the "molecule" table.

📋 Step 2: Extracting dataset definitional rules...
Dataset Rules:
Condition: When the question mentions "carcinogenic"
Operation: "carcinogenic" actually means "T2.label = '+'" in schema.

Condition: When the question mentions "carcinogenic"
Operation: use T2.label = '+' as the condition for carcinogenic molecules

Condition: When the question mentions "carcinogenic"
Operation: "carcinogenic" actually means "molecule label = '+' in schema".

Condition: When answering about "carcinogenic molecules and their elements"
Operation: make sure the output order: T2.molecule_id, T1.element

Condition: When the question mentions "carcinogenic"
Operation: "carcinogenic" actually means "T2.label = '+' in schema"

Condition: When the question asks for "distinct molecules"
Operation: compute the number of unique values of T2.molecule_id

Condition: When the question mentions "carcinogenic"
Operation: "carcinogenic" actually means "label in schema" with values like '+' and '-'.
  ``` 
  ```
  Sample 91/100
  Database: california_schools
  Question: For the school with the highest average score in Reading in the SAT test, what is its FRPM count for students aged 5-17?
  Ground Truth: SELECT T2.`FRPM Count (Ages 5-17)` FROM satscores AS T1 INNER JOIN frpm AS T2 ON T1.cds = T2.CDSCode ORDER BY T1.AvgScrRead DESC LIMIT 1

  🤖 Step 1: Generating rules with fine-tuned model...
  Generated Rules:
  Rule 1:
  Condition: When the question asks for the school with the highest average score in Reading
  Operation: use the "AvgScrRead" column from the "satscores" table to determine the highest score.

  📋 Step 2: Extracting dataset definitional rules...
  Dataset Rules:
  Condition: When answering about "school with the highest average score in Reading"
  Operation: rank schools by T1.AvgScrRead in descending order and keep the first 1 row.

  Condition: When the question asks for "FRPM count for students aged 5-17"
  Operation: use the exact column T2."FRPM Count (Ages 5-17)" instead of FRPM_5_17.
  ```
  ```
  🤖 Step 1: Generating rules with fine-tuned model...
  Generated Rules:
  Rule 1:
  Condition: When the question asks for race dates
  Operation: select "date" from the "races" table instead of "race_date" from "formula_1".

  📋 Step 2: Extracting dataset definitional rules...
  Dataset Rules:
  Condition: When answering about Formula 1 race dates on Barcelona-Catalunya circuit
  Operation: make sure the output order: T2.date

  Condition: When the question mentions "Barcelona-Catalunya circuit"
  Operation: "Barcelona-Catalunya" actually means "T1.name = 'Circuit de Barcelona-Catalunya'" in schema
  ```

- 改进：训练时去掉没有definational rules的数据
- 可以先试试用qwen 凝缩definational rules，便于作为lable。不然lable长度不一定而且含有重复大量无用的信息



## 9.24
- 
