## 11.1
- RecoverExerciseAPP整体用kotlin重构，先构建一个能跑起来的再说

## 11.3
- 下游配比实现，上游聚类调研想方案
- 先复现配比实验
  - DoReMi
- 用coder微调数据集的各种分类角度做可行性验证：
  - 难度
  - 语言
  - 长度
  - ......
- 论文可以看r&b

## 11.4 11.5
- R&B:
  - 实际上R&B对大问题做过相似的形式化定义，但是由于停留在浅层离散聚类方法，把大问题定义为成本过高“不可解决”，退而求其次将其拆解为两个不连续的子问题（聚类，配比）
  - 
- 仿照doremi实现微调版本proportioner  
  - 这是后续实验的下游基础
  - 实验数据可以用作引入当前的语义聚类角度的多样性，以及最佳语义角度的不固定性，进而批判R&B等论文的做法
  - 数据预处理：
    - pass
  - 设计：
    - 使用32M预训练过的小模型做proxy和reference，任务为微调，Kodcode生成代码任务，指标为默认的生成token交叉熵损失，生成最佳配比
    - 以默认数据配比和训练数据配比微调8b预训练模型对比结果交叉熵损失和执行率，结果准确率
    - 微调reference用1000条测试数据，生成配比阶段全部数据参与，后面对比训啊林提取1000条左右数据的测试集
  - Qwen2.5 0.5B
    - testSet(3000):Average evaluation loss: 0.7821
  - reference model
    - train 10 epochs on 5000 train data
    - testSet(3000):
      - 0 epoch：Average evaluation loss: 0.8321
      - 2 epoch：Average evaluation loss: 0.7947
      - 4 epoch：Average evaluation loss: 0.8894
      - 7 Average evaluation loss: 1.0968
      - 9 Average evaluation loss: 1.1866 
  - Qwen3 8B
    - Average evaluation loss: 0.5233
    - 
  - 微调改写，微调需要的参考模型参数量需要大一些
- 横向RosLM:
  - 搞明白这是啥玩意并尽量部署起来
  - libFuzzer 生成代码输入，对代码进行测试，可以查看本次输入触发了哪些代码段的执行，并保存作为种子
  - 覆盖率：测试用例触发的代码占比
    - 行覆盖率
    - 分支覆盖率
    - 边覆盖率，实际被使用，因为实际上的代码应该视为一个图
  - 插桩： 编译时在代码中加入路径纪录点
  - 利用-merge参数，还可以实现有效语料的提取，即从一批候选语料中，找到能够触发新代码路径的语料
  - Fuzzing Harness：负责输入格式转换以及调用和错误处理
- r&b的实验图




- Ros问题
- 用什么大模型
- 问题：
  - 训练大模型 是否要改，大模型生成的东西只有种子还是跨过fuzz把用例全部用模型生成
  - 测试用例语义语法检测是什么意思，







## 11.8
- R&B提出了联合优化的形式化定义，使我们的问题不够创新
- 需要深入挖掘新的角度，继续升华
- 原题目：
  - 使用深度聚类，先聚类后配比，以训练效果为目标联合优化深度聚类的表示空间和进一步的配比
- 联合优化的问题已经被提出（虽然还没有被解决）
- 假如划分配比同时进行？
- 假如先配比再划分？或者配比划分同时进行？
  - 配比限制为：模型有多需要这类（能产生这种梯度的）数据？
- 只配比：
  - 划分可能不是最优
- 独立划分，配比的坏处：
  - 划分与结果与训练效果脱钩
  - 效率差，没有可用性
- 先划分，再配比有什么坏处（新的问题是什么）？
  - 首先无法做到联合优化（计算成本高，还不如直接训练）
  - 这种方法分为两种，先聚类
    - 一种使用小的代理模型+excess loss计算固定配比，使用这个固定配比训练模型。
    - 一种是一遍训练一遍调整不同类的配比
    **这两种都没办法做到根据模型的需要动态变换聚类策略，类比于学习过程中的重点以及重点的划分角度是不一样的**
- 配比的同时动态划分可以解决这个问题




模型训练过程中的数据偏好不是一成不变的，这包括了数据配比（doremi）和划分（R&B）都应该是动态的
- 重点不应该再是我们同时考虑了配比和划分
- 而是在于：如何尽可能细粒度的，动态的在训练过程中挑选最合适的数据
- 现在的工作只考虑**训练数据选择**过程中的一部分模块，要么只考虑默认领域划分的配比（Doremi），要么在一个优化后的固定划分下进行配比（R&B），但是这种先产生唯一划分，后配比的想法天然的就不适合模型在训练过程中的动态成长，事实上，模型训练过程中，偏好的数据划分角度和配比方式都应该是动态变化的。
- 而是我们**融合**了配比和划分，从更细粒度的角度提出新的问题：
  - 怎么在模型的**训练过程中**根据其偏好**动态划分输入数据的类型和各类型的配比**

- doremi：
  - 无聚类，只有配比
- R&B（ideal）：
  - 找到一个最佳聚类，然后在最佳聚类下动态配比
- R&B(fact):
  - 单一聚类表示方法下的近似最优聚类，然后动态配比
- CODR：
  - 动态聚类的同时配比




### 题目修改
增加一个创新点，并改变切入角度为三方面：

模型的训练数据选择与配比是一个热门研究方向


- 目前绝大多数研究，将数据集给定的默认数据领域划分看作理所应当，没有考虑配比前重新聚类。
但是默认的聚类不一定是最优，而在次优的聚类方式下的配比也一定是次优的。


- 对于目前极少的同时考虑聚类和配比的方法，聚类和配比也是解耦的，不基于模型的反馈。

- 比较特别的是以R&B为首的研究。尽管这类研究理论上提出了聚类和配比与模型的训练效果相挂钩的概念，但是其聚类结果是单一固定的，一个数据集只有一种聚类，和模型训练过程脱钩（而且没有实现，仅从理论证明了解耦的近似解法
根本原因：
  - 目前所有先进的方法将划分和配比看作两个阶段的独立模块，但是这种先产生唯一划分，然后配比的想法天然的就不适合模型在训练过程中的动态成长。事实上，模型训练过程中，偏好的数据划分角度和配比方式都应该是动态变化的。


- 此外，目前的训练数据聚类都是在模型默认的嵌入空间表示中进行浅层聚类，聚类的表示和结果都单一，无法捕捉深层可变的关系。特别是当数据的相似度高，训练目标隐含时。


我们认为：
- 聚类和配比应该是动态绑定的，在训练过程中一起根据模型的梯度更新动态迭代，对于数据选择过程来说同属于一个原子操作
通过解决：
1.怎么联合优化数据的划分和配比，并将划分-配比融入到模型的训练过程中动态迭代
2.怎么根据模型的反馈，结合深度聚类，找到当前最合适的数据聚类表示
解决了以下问题：
1. 默认领域下配比效果提升有限（划分-配比联合优化（挑战是划分对配比离散反馈
2. 划分-配比与训练效果解耦（划分-配比动态融入训练过程
3. 单一语义空间聚类能力有限（梯度反馈驱动深度聚类
4. 划分-配比-训练效率底下，计算成本高昂，无可用性


总之，CODR的贡献是：
- 1. 实现联合优化配比和聚类，在训练迭代中寻找最优划分下的最优配比。
- 2. 首次提出并实现数据划分随模型训练的反馈动态进行，适应模型动态演化的数据理解能力，每次都能在适合模型当前能力的领域上进行配比。
- 3. 提出模型训练梯度反馈驱动的深度聚类框架，首次应用深度聚类在模型训练数据领域，全面、动态捕捉训练语料的聚类表示。
- 4. 有性能上限，做到可用





总结：
- 由于绝大部分没有提出联合优化，唯一提出联合优化的
- 对于R&B，不攻击他的划分不是动态的（尽管目前没有研究能做到真正动态划分）
- 攻击他的划分是唯一的，配比在静态的划分下进行，没有结合模型实际训练中划分角度的迭代
- 
- 划分的问题：
  - doremi等大部分方法：只考虑配比，不考虑划分
  - R&B等方法：考虑配比和划分，但是划分是




## 11.9
- CODR
  - proportion only方法是第一个切入角度：
    - How can they be sure if the default domain clustering is best? 怎么就敢确定默认领域划分就是最好的？
    - 没有好的划分就没有好的配比,划分-配比应该是联合优化问题
  - Shallow Clustered by Skills是第二个切入角度：
    - 浅聚类在默认语义向量下，默认会以垂直技能角度划分数据，凭什么假定从默认空间表示的角度（技能）划分是最好的？
	  - 应该在每一轮训练时，模型需要从哪个角度划才怎么划(用深度聚类捕捉聚类的不同角度)
  - One more step: Why one angle at a time? 是第三个切入角度
    - 为什么要固定同一批数据只能有唯一的最优聚类角度？
    - 应该不同角度，多层次聚类
  - 性能和划分-配比粒度是第四个切入角度

  - 本研究认为：
    - 聚类和配比应该是动态绑定的，在训练过程中一起更新动态迭代，对数据选择过程来说同属于一个原子操作
  - 由此提出自顶向下的聚类配比树作为新的解决方案（拟），希望同时解决：
  	
  	- 1. 数据的划分-配比联合优化
  	- 2. 找到每一批次数据最优划分角度下的深度聚类（新）
  	- 3. 同一批次数据能有不同角度的多层聚类（新）
  	- 4. 性能问题



## 11.10
- 论文
  - 本质上是在别人挖的坑上再挖一勺（划分-配比）应该弃坑（艹）
  - 太难了，顺着这个搞？
  - 能不能换一个角度，内容不变，角度升华，说明我们不是挖一勺而是开新坑
  - 不能是“划分的角度”而是和“划分”平行的一个概念：分配
  - 深度数据分配
  - data distribution for LLM training data
  - abstract:
    - 大模型训练数据的配比操作是一个热门的研究方向，现有的训练数据分配方法使用简单语义聚类，并将聚类和配比看作两个完全独立的模块，在固定唯一数据划分角度的同时
  - 还是感觉不行，发现配比之前需要划分，这个事情让别人干过了，即使找到更好的动态划分办法，也不是开新坑
  - 但是我的数据不一定够做配比
  - 动态调整loss呢？
  - 换一个题目
  - 


- 横向
- RosFuzzer
  - 没有现成的种子生成模型
  - 两个不同的模型，分别负责c++和python模块




## 11.11
- 新题目：动态微调数据配比，对模型重训练，以适应目标迭代
- 场景，解决的痛点：在数据集S={s1,s2,s3....}上训练的模型T1,应用一段时间以后，优化应用场景为S'（相比S做增删改操作）这个时候对S'从头训练一遍模型耗时耗力，因为S'与S的任务重叠很大。期望找到一个配比方案S'',对T1进行二次微调（或其他方案），达到S’的期望。
- 理论支撑






好的，以下是根据论文内容整理的实验数据集和对比方法的介绍表格。

### 表一：实验数据集

| 数据集名称 | 类型 | 来源与描述 | 数据规模（平均） | 特点与难点 |
| :--- | :--- | :--- | :--- | :--- |
| **Web Tables (WT)** | 真实世界 | 来自Google Fusion Tables，包含31对来自17个不同主题的表格。 | 92.13行/表，31字符/行 | 包含自然噪声和格式不一致，部分实体无法用传统字符串变换处理，是具有挑战性的基准。 |
| **Spreadsheet (SS)** | 真实世界 | 来自Microsoft Excel用户帮助论坛和产品团队，包含FlashFill和BlinkFill的公开基准。 | 34.43行/表，19字符/行 | 噪声较少，格式相对规范，主要涉及电子表格中的数据清理问题。 |
| **Knowledge Base Web Tables (KBWT)** | 真实世界 | 来自知识库（KB），需要语义转换和额外KB信息。 | 113行/表，13字符/行 | 目标转换依赖于语义理解而非纯文本模式，与WT数据集有显著区别。 |
| **General Synthetic (Syn)** | 合成 | 通过随机组合3-6个基本变换单元（如子串、分割等）生成。 | 10张表，100行/表 | 输入长度8-35字符，无人工噪声。用于测试模型对未见过的复杂组合变换的泛化能力。 |
| **Easy Synthetic (Syn-RP)** | 合成 | 通过随机替换一个字符（如`/`替换为`-`）生成。 | 5张表，50行/表 | 模拟简单的格式更改（如电话号码中的符号替换）。模型在训练中未见过此操作，但所需编辑操作很少，故难度为“简单”。 |
| **Medium Synthetic (Syn-ST)** | 合成 | 对输入源应用单个`substring`变换单元生成。 | 5张表，50行/表 | `substring`是模型训练中见过的操作。根据所需编辑操作数量，难度定为“中等”。 |
| **Difficult Synthetic (Syn-RV)** | 合成 | 通过反转输入字符串中的所有字符生成（如"Hello" -> "olleH"）。 | 5张表，50行/表 | 需要改变几乎所有字符的顺序，且模型在训练中未见过此类变换，故难度为“困难”。 |

---

### 表二：对比方法

| 方法名称 | 类型 | 核心思想 | 优点 | 缺点/局限性 |
| :--- | :--- | :--- | :--- | :--- |
| **Common String-based Transformer (CST)** | 基于字符串变换 | 利用源和目标示例之间的公共文本序列作为证据，形成基于子字符串操作的变换骨架。 | 能处理一定噪声，相比Auto-join有更好的运行时性能。 | 仅限于预定义的子字符串操作，无法处理无复制关系的变换（如字符反转），在复杂情况下搜索空间仍然很大。 |
| **Auto-FuzzyJoin (AFJ)** | 基于相似度匹配 | 使用一组相似度函数（如编辑距离）来检测最可能连接的源和目标行，无需标注示例。 | 对于源和目标文本相似度高的情况非常有效。 | 严重依赖源和目标之间的文本相似性，在两者差异巨大时（如反转任务）性能急剧下降。 |
| **Ditto** | 基于预训练语言模型 | 对预训练语言模型（如DistilBERT）进行微调，用于实体匹配任务，利用语义相似性进行判断。 | 能够捕捉语义层面的相似性，在匹配任务上表现强大。 | 同样依赖于源和目标之间的相似性（语义或文本），在缺乏这种相似性时性能不佳。模型是为匹配而非生成设计的。 |
| **GPT-3 (Curie)** | 大型语言模型 | 在少样本设置下，将示例和待转换源行作为提示词输入模型，直接生成目标值。 | 在拥有海量先验知识，尤其在真实世界数据上，提供足够示例后表现接近DTT。 | 在合成数据（尤其是随机字符和非自然变换）上表现不佳。直接使用时不具备DTT的分解与聚合机制，可能不稳定。 |





## 11.13

- 新题目：mixing feature-heterogeneous data for one specialty



## 11.15
- RosFuzz



## 11.16
- 先想问题定义
  - 先想场景，以前的论文。。。
  - 然后提出一个新的角度，应该是怎样。。。。。
- 然后想解决方案。。。。



## 11.17
- 基于模型反馈的配比，
  - 有人做过：
    - 聚类对配比的影响
    - 随着模型训练过程，动态的配比
  - 没有人做过：
    - 不同的聚类角度对配比的影响
    - 动态的聚类联合配比
    但是聚类的影响已经被探讨过，而且动态的深度聚类难以实现
- 基于训练目标的配比（偏向数据选择：
  - 已有框架：
    - 表示：
      - raw text和向量降维表示都有
    - 标准：
      - 数据集自身对照
      - 外部模型评判，或者外部模型预测训练loss
    - 评估
  - idea：
    - 如何定义“好的训练数据”   （舍，与目标集之间的差距有一篇LESS用梯度相似度做过了
    - 配比目标之间的相互影响（需要验证R&B是否考虑到了）
    - 似乎把数据选择和配比分开来了？选择是选择高质量符合目标的数据（最多关注一下多样性），配比是使难学的梯度影响大的多。
    - 这样会有什么问题？
      - 选择时只考虑单条数据，配比时数据量可能与任务目标不符
      - a+b+c，合在一起可以提升能力1，a+c+d，又可以提升能力2，
      - 面向能力配比的数据选择+配比
    - 重点论文：SHED LESS CaR

## 11.18

- 新想法：面向模型能力的微调数据配方生成框架

  - 整体流程:
    - 用户用自然语言描述自己想要的目标模型，有哪些场景，哪些功能，并提供few-shot data example。描述场景和example越详细，建模模型分析就越全面。
      - ‘我想要一个物理学家模型.......’
    - 模型分析建模目标模型的能力空间:
      - $S$=[数学建模,抽象思维,计算能力,实验设计,批判性思维,专业软件使用,仪器操作,编程与算法,量子力学，经典力学，]
      - 或者一个更有层次感的例子：
        $S$=[语言能力，逻辑思维，编程能力，汇编语言能力，高级语言能力，pyhton，c++.......]
    - 模型分析目标模型在该空间中的位置向量表示：
      - $P_{target}$=[数学建模:0.1,抽象思维:0.5,计算能力:0.9,实验设计:0.3...]
    - 对于数据集全集$D$中每一条数据，模型都能抽象成在该能力空间中的梯度，实际上就是对与每一个能力维度进行打分，不一定是正数:
      - $d_{one_data}$=[数学建模:0.9,抽象思维:-0.2,计算能力:0.2,实验设计:0.7...]
    - 取子集，对预训练模型进行评估，估计模型初始在该空间中的位置向量
      - $P_{origin}$=[数学建模:0.05,抽象思维:0.1,计算能力:0.3,实验设计:0.0...]
    - 在以上条件下，就能精确，可解释的规划如何利用数据集$D$对应的梯度集${d_1,d_2.....}$使模型从$P_{origin}$到$P_{target}$
    - 这个数据梯度的逼近过程就可以作为该训练的数据配方

- 原本：
[]
- 改进：
[]

## 11.19
- 面向模型能力配比的微调数据配方
- 传统数据配方：
  - 数据选择，标签固定，面向质量，难度，多样性，目标任务分布对齐
  - 数据配比，面向模型掌握知识难易
  - 配比和选择分离，本身只有在目标分布对齐的过程中才考虑目标知识领域对齐，缺乏挖掘模型的目标能力本身，且在层层处理过程中更忽略这一点
- 在微调之前应该有一个理想的模型描述，需要这个模型擅长在哪些场景做哪些事，但是这个关键信息在后面的过程中被抽象成为一个简单地测试集，或者简单、离散的目标知识领域分布。
- 提出面向模型能力的微调：面向微调设计者，保留“需要模型在哪些场景做哪些事”这个关键的描述信息，训练模型从这个信息中挖掘目标模型的能力空间，并在这个空间中对数据和预训练模型建模，从全集中输出数据配方，更快更好训练目标模型。

- abstract
  - 大模型微调的数据配方是一个热门的领域，旨在选择和配比数据，用更少的数据和时间微调出更符合微调目标的模型。然而，目前的数据配方方法用固定的评价指标筛选不同领域的高质量数据（选择），再将对模型影响大的知识簇配比调高（配比）。这不但需要在提前调整好目标领域的分布

- simplify，数据配方，标签
- 根据用户指定的微调任务目标，动态构建数据评估标准，实现一站式高效数据配方
- 可用性
- 效率
- 质量效果
- abstract
  - 大模型微调的数据配方是一个热门的领域，旨在选择和配比数据，用更少的数据和时间微调出更符合微调目标的模型。其中最关键的是衡量数据本身的质量的标准尺度，即，从哪些角度对数据进行选择和配比。然而目前的知识配方，数据配方标准大多是固定通用的。我们提出......
- intro
  - 数据配方的标准是指评估数据的尺度，常见的尺度包括难度，领域，多样性，与任务目标的对齐程度等等。
  - 常见的数据配方包括数据选择和数据配比两个阶段。SOTA数据选择方案的尺度通常固定为准确性、相关性、难度、信息量和多样性等，而SOTA数据配比方案的尺度通常固定为对模型的影响大小。也就是说，对于不同的微调任务，一个配方流程看待数据的尺度是一成不变的，而且不同的阶段的尺度无法统一。
  - 这样会造成的问题：
    - 缺少针对性，训练物理学家模型和文学家模型使用同样的数据选择尺度
    - 提升了实现高质量微调的难度，为了能与微调目标任务对齐，用户需要自行配比不同技能领域
    - 
    - 效率损失：配比标准和选择标准不统一
  - 我们



- 针对数据配方，不同的配方方法和阶段评估标准不统一的问题，提出一站式解决方案

- 给定一个数据集


- 动态数据评估
- 评估维度建模模型
- 背景：数据选择-配比：AlphaGasus（ICML 2024）使用GPT对数据进行评分选择，其评分维度由人工指定。
- 问题：不同的目标模型应该有不同能力评估维度，对应的也应该有不同数据评估维度，这个维度




## 11.22
- 横向
  - 为什么模糊测试的覆盖率对driver要求很高？
    - 
  - 找没有循环迭代那种复杂操作操作，用户友好的方法（偏向工业解决方案
      - test-agent：
        - 2023年的，很简单的专门生成测试用例的微调模型，开源，7b
        -  https://github.com/codefuse-ai/Test-Agent?tab=readme-ov-file
        - 为“傻瓜式开箱即用”提供示例
      - oss-fuzz-gen
        - google2024的开源LLM写driver方案
        - 只支持openiai和Gemini，需要改源码

  - 想经济的可以被接受的训练方法，他们想不到的
    - 

  
  - 公众号科研，看他发的公众号论文
    - 公众号里的论文题目和内容不一致
    - A transformer-based framework for software vulnerability detection using attention-driven convolutional neural networks
      - ccf c
      - CodeGATNet（基于卷积特征的代码门控注意力网络）的新型漏洞检测技术啥的
    - DeepVulHunter: enhancing the code vulnerability detection capability of LLMs through multi-round analysis
      - Future Generation Computer Systems (FGCS)
      - 构建漏洞数据库
      - 找到目标代码的相似代码
      - 执行漏洞分析，输出漏洞报告
      - 只发现漏洞，不测试
    和要求的不是一个东西
  - 
  - 测试用例指什么？driver还是语料还是driver和语料的组合？



- 论文：


  - 情况补充：
    - 如果没有用户输入，也没有验证集：
      - 给一个默认prompt，要求框架挖掘给的训练集中的数据，猜测用户的训练意图，然后把合成的训练目标传给能力建模模块

      也就是说默认的pipline就是直接分析训练集，用户的可选输入替代是锦上添花的作用

    - 如果用户输入很弱：
      - 提醒用户平均的能力配比会造成“什么都能做，什么都做不好”的情况，应该根据实际应用场景配比，并给出配比建议。

  - 问题拆解：
    
    1. 目标提炼模块 或用户输入: Object distillation   
    2. 能力空间建模：Space Construction
      - 要求有一个方法能对整个数据集有宏观统计，每条数据的tag，估计意图，以及意图与实际是否符合
    
    
    

    3. 目标模型定位: Text Projection 
    4. 预训练模型定位: Model Projection
    5. 数据梯度化: Text Projection
    
    6. 梯度逼近策略: 

  - 特殊的维度：
    - 相似度，与fewshot example或者验证集

  - 目前最大的问题：怎么准确从一批数据中找到用户的训练意图？没有用户输入时怎么预测用户目标模型，效果还能超过sota？
    - 不用超过，基本持平就行，只是有这个“没有用户输入也能跑”的解决方案，重点还是在有用户输入或者验证集时上
    - 那怎么总结意图？或者说更基础的，建模时也需要对整个数据集有一个总览
  

  - Object distillation
    - 打标签



## 11.23

- 横向：
  - ossfuzz gen
    - 暂时跑不起来，少gemini api
    - 原理：使用oss fuzz intropector发现核心api，给LLM写driver，没有循环变异操作
      - introspector：
        - Promptfuzz读取头文件以获取所有api，通过能量分数选择每个api被放进driver的概率
        - 圈复杂度是覆盖全部路径所需的最小的测试用例数量，用例值的就是输入的语料
        - introspector会统计代码的每一个函数的圈复杂度，分支，流程依赖关系图，优先建议复杂度高的分支多的api
        - introspector会先进行简单fuzz，运行时分析代码覆盖，如果有一部分代码一直没有被覆盖过，他会分析是哪些逻辑阻碍了比较路径，并返回一个html报告
        - 这个流程没有调用模型
        - 输出示例：
          - 
        
  - promptfuzz 简化
    - 生成的fuzzer应该是到不了1s一个但是也有5-10s
    - 编译通过率应该没问题，主要都是被sanitasor杀死的
    - 最大的问题在需要一个大参数的模型

  - 新公众号论文
    - 生成
- 




## 11.24
- 横向
  - 用大模型生成driver，直接输入需要修改的代码，返回driver，把api选取等部分融进模型内部，类似
  - 解决公众号论文2所说的：现有模型找漏洞依赖模式匹配，没有逻辑推理的问题
  - 核心：现有都是面向api迭代生成driver，没有使用微调的模型来分析代码的。可以尝试使用模型分析代码直接输出driver
  
  - 写完项目申请第二部分
  - 看完CKGfuzzer

- 论文：
  - 题目完成，调研方法
  - 目标模型描述的模型直接用预训练模型做就行，因为只用一次
  
  - 找打开放tag的原始论文 