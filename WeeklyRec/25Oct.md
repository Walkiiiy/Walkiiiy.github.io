## 10.7
- 对cleaned length rules训练9个epoch的mixed model进行测试
  - 13293: loss 0.095
  - 12000: loss 0.093
  - 11000: loss 0.091
  - 10000: loss 0.086 ex:52 40
  - 9000: loss 0.0887
  - 8000: loss 0.0953
- 失败，不稳定，且由于调高schema关键字loss，模型会刻意生成没有意义的schema关键字组成的字符串。进步在于loss降到0.0几
- 用gpugeek调30bmoe qwen3 试试


## 10.9
- 模型训练结果测试
  - 9942:0.8366
  - 9000：0.8403
  - 8000：0.8283
  - 7000:0.8250
  - 6000:0.7328 0.6597
  - 5000:0.7426
  - 4000:0.7535
  - 3000:0.7371
  - 2000:0.7298 0.6677
  - 1000:0.7612
- 对所有sql执行保存结果。
- schema-linking 加到assumer，再训练
- 检查deepspeed脚本

- 弱化所有question，测试结果
- 读arcticsql论文，考虑将arctic R1作为generator


## 10.13
- Spider 是11个人工编写NLQ，但是是计算机专业人员根据db编写，也就是事先看过行列内容以后编写，即使后面有用英语母语者复述的过程，也只是增加语义自然度。
- Bird
- text2sql最初
- 我们可以生成大量不根据DB的自然语言问题(说是人工生成)，模拟自然场景下人工问的问题。这种场景下必然会生成DB信息不足无法解答的问题，然后我们要求模型能判断DB的信息是否支持解答这个问题？或者结合Assumer解答的置信度有多少。

- schme-linking
- schma-linking_extraSchemaInfo

## 10.14
- test10.13assumer
  - steps execution
  - 1000  0.4647
  - 2000  0.5495
  - 6000  0.5357
  - 7000  0.5394
  - 7104  0.5348
  - 10000 0.541
- 之前的低可能是因为target rules起到了一定的schema-linking作用？ 
- 不可能用了schema-linking效果还没以前好，只有可能是轮数不够。
- 上强化学习
- 先分隔数据，训练模型根据condition生成一条rule，然后上vllm agent（localAssumer）


## 10.15
- DTT
- word2vec,subword,bbpe,byt5

## 10.16
- 远离LLM通用性强的热门的大问题，解决小而关键的问题
- 快速验证可行性
- 三种论文：
  - 提出新的问题定义benchmark
  - 提出问题定义的新解决方法（最好是做的人少的问题，又引入创新性的方案，最容易）
  - 同时提出新的问题定义和新的解决方法（最难）
- 做的人少就意味着数据和相关工作难找，但是剩下的相对容易
- 现在是有具体的新的题目，还是还需要调研
- 如果还需要调研，可以考虑LLM在表的某个细分领域的应用
- 目前想到的有排序，拆分，筛选，与其他领域不同的是，表数据需要考虑多列因素，而且含有的语义信息相比较少甚至没有

- 或者就做这个题目:
  - 把examples和schema-linking结合起来
  - 把sematic和none-sematic结合起来，或者再找其他角度
  - 生成目标不做join，做填补或者纠错

- 题目：
  - 有一个大文档，里面有很多表格，表格的表头是抽象的，文档中有两种信息可以解释表头：
    - 直接定义A=...
    - 在文档中有B的解释，但是没有直接提出B而是需要模型提取总结来理解 
    然后对进行相关问答
  - 定义新的问题的同时提出新的成熟的解决方案
  - 合成新的数据集，并把已有的方案（用于解决其他相似问题的）强行移植到这个方案中来

- 问题答案：
  - 存在文档中
  - 存在文档表格中，表头清晰
  - 存在文档表格中，表头不清晰，定义在文档
  - 存在文档表格中，表头不清晰，隐含在文档
  - 不存在文档中
- 数据集合成方案：
  - wiki-table 变形


## 10.17
- 两种表格变形：
  - origin->A,b,c
  - orogin->shortened names
- 多表联合查询：
  - 把已有问题的目标表拆分？
- 最好还是先找已有数据集进行修改，不行再自己合成问题



## 10.19 10.20
- 抽象表格示例：
![abstructTable](abstructTable.jpg)
- 题目定义：给定含有表格的文档和一个表格相关自然语言问题
  - 传统：文档序列化表格（如md），向量化存储，rag，问答
  - 问题：当表格中含有抽象信息，而这些抽象信息只有结合文档主体信息才能解释时，模型能力不足。
  - 数据集合成：
    - 从NTRL找技术报告和项目进展报告（1024）
    - 格式化pdf，mineru提取表格
    - 提取表头信息（需要先清洗一遍，去掉数值日期等类型，或者直接用表格结构识别模型再处理一遍）
    - 表信息给本地模型，模型来判断是否抽象，筛选不抽象的表格，并指出具体的抽象表头关键词
      - 抽取前n行数据
      - 'you are a professional data analyzist.'
      - 'You will be given a table, please combine the header and content, divide the table into one of three types:'
      - '1. tables that are clear and easy to understand.'
      - '2. tables that are abstract or confusing, needs other information's help to understand. If the table belongs to this type, please state the abstact keywords.'
      - '3. tables that can's be understood or doesn't even look like a table.'
      - 'please return in strict json format like:{type:1,abstract_keywords:[...]}, do not return any other content'
      - 'the given' 
    - 含有抽象表格的文档进行rag问答，对抽象表头进行解释，生成详细表头
    - 将抽象表头替换为详细表头
    - 替换后的文档送入模型，要求模型提出与被替换关键词相关的问题和答案
    - 将文档表格中的关键词替换回抽象关键词，与问答对生成数据集
  - 问题存在证明（比较实验）：
    - 表格专用问答模型：
    - rag专用模型：
    - rag-表格专用模型：
  - 方法：
  - ps:
    - 问题专注于找到表格与文档的连接，问题本身的逻辑应该设置得简单一些（select类）


- 问题：方案规模？可以是中间件，加上可以增强跨模态推理，或者也可以直接做一个RAG框架


- idea: 自问自答专用模型


## 10.21
- 对抽象关键词数据进行二次清洗
  - 清洗前 754个pdf中提取 8674个表格
  - 清洗后  754个pdf 7804个表格
  - 再清洗  470个pdf， 5245个表格
  - 再清洗  470pdf， 5194 个表格
- 读完tableRAG，了解背景方法

## 10.22
- 先解决实验思路问题
  - 8b提取的关键字正确率不高：
    - 有一些没有被提取，有一些不是表头，有一些不抽象
  - 改用32b，效果差不多
  - 人工，保守估计5000个表格，一天1000个要5天
    - 看表格图片判断是否抽象
    - 抽象关键字再在文档中找解释，标注上详细解释

- 问题具体化，对于一个模糊关键字：
  - Doc中有清晰解释
  - Doc中没有解释，但可推理
    - 通过表格行列上下文推理
    - 通过Doc上下文推理
  - 不可推理

- ppt



## 10.23
- 根据实验，几乎所有有直接映射的情况，大模型都能完美回答，而绝大多数的抽象关键字在文中都有直接映射。
  - 要不要把题目修改成抽象关键字在文档中完全没有直接解释，需要结合文档和表格上下文理解
  - 但是这样数据集的合成要求又会提高

- 新大方向：LLM训练数据质量：
  - 数据清洗纠错填补
  - 数据生成
  - 数据配比


- 数据配比
  - 特定领域微调时，也需要领域内的通用性，而微调所用的数据却来自有限的不同的领域（比如text2sql，表格问答，传统文本问答，微调coding专用模型，微调的数据库来自所有不同的领域）
  - 目的：已经聚类的n个领域的高质量数据，要求使用给定有限的数据总量，找到最合适的各个领域的训练数据配比，以最大化训练后的通用性
  - 标准：回答训练所用所有数据集领域外的新领域问题的正确率
  - 前提假设：所有领域的数据质量相当
  - 为什么要数据配比？有那么多的数据为什么不全用上？

## 10.24
- 配比方法：
  - 输入：
    - 训练数据集，要包括领域信息如：
    ```
    {
      {
      'domain':...
      'num':...
      'data':[...]
      }
      ......
    }
    ```
    - 模型参数/模型本身
    - 下游任务接口
  - 输出：
    - 最佳训练数据配比
  - 和多参数函数在给定条件下的最大值的关系


- 背景调研：
  - 多数配比研究都是上游预训练过程，以em和step为标准
  - 下游有少量研究，都是针对微调多任务模型，也就是配比微调训练数据能使模型同时擅长多个领域
  - 没有单一领域内的数据配比




## 10.26
- data-jucier
  - 重点算子op：
    -  instruction_following_difficulty_filter： 使用IFD分数（带有查询条件下的损失与无查询条件下的损失之比。只有IFD分数在0.2到0.9之间的样本被保留），同类型的还有llm_difficulty_score_filter，
    -  word_repetition_filter：n-grame重复比率在一定范围内
  - 提出数据HPO：数据处理的配置本身（如过滤阈值、数据混合权重、清洗强度）是一类新的、对模型性能有关键影响的“超参数”，我们称之为“数据超参数

## 10.28
- 聚类算法
  - k-means 
    - 预先指定k个簇心，不断将样本放到空间中并更新簇心使所有数据到簇心的平均总距离最近，直到收敛
    - 需要预设k
  - 层次聚类
    - 凝聚型：不断合并最近的两个簇
    - 分裂型：从整体不断分裂
  ......
- 距离函数动态优化的聚类算法————度量学习驱动聚类




## 10.29
- 深度聚类，文本输入：
  - Sccl
    - small-batch input
    - 投影头被两个头的loss联合优化
    - 对比学习头负责优化表示空间，去除overlap现象，每一个样本增强一个copy做正样本对，输入loss是infoNCE
    - 聚类头负责聚类，无监督学习，产生的软标签用学生分布t归一化，归一化后产生的软概率向量v和经处理的v用kl散度对比，结果为loss
    - 问题：
      - 为什么infoNCE不会把相似度高和低的样本拉到同样远？
      - 这个聚类目标是怎么保证相似度高的样本在同一簇的概率大的？

## 10.31
- RecoverExerciseAPP:
  - 使用 trjs改写已有的懂多比较，集成到expo