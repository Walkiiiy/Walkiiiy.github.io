Certainly! The paper introduces two key variants of **Retrieval-Augmented Generation (RAG)**: **RAG-Sequence** and **RAG-Token**. Both combine retrieval with generation, but they differ in **how retrieved documents are used during text generation**.

---

## üîÅ RAG-Sequence vs. RAG-Token

### üß± Shared Structure

Both models consist of:

* A **retriever**: finds the top-*k* relevant documents (passages) given an input (query).
* A **generator**: generates an output sequence (like an answer or sentence) conditioned on the input and the retrieved documents.
* A **probabilistic formulation**: treats the retrieved documents as **latent variables** and marginalizes over them.

---

## üß© **1. RAG-Sequence Model**

### ‚ûï **Key Idea**:

The **entire output sequence** is generated using the **same retrieved document**.

### üß† How it works:

* Retrieve top-*k* documents `z‚ÇÅ, z‚ÇÇ, ..., z‚Çñ` for input `x`.
* For each document `z·µ¢`, compute the probability of generating output `y`:

  $$
  p(y | x, z·µ¢)
  $$
* Final output probability is the **weighted sum** over all documents:

  $$
  p(y | x) = \sum_{z \in \text{top-k}} p(z|x) \cdot p(y|x, z)
  $$
* One **document is responsible** for the whole sequence during decoding.

### üü¢ **Advantage**:

* Simpler decoding and beam search.
* More stable and efficient for short-form generation.

---

## üß¨ **2. RAG-Token Model**

### ‚ûï **Key Idea**:

Each **token** in the output can be generated using a **different retrieved document**.

### üß† How it works:

* For each output token `y·µ¢`, compute:

  $$
  p(y·µ¢ | x, y‚ÇÅ:i‚Çã‚ÇÅ) = \sum_{z \in \text{top-k}} p(z|x) \cdot p(y·µ¢ | x, z, y‚ÇÅ:i‚Çã‚ÇÅ)
  $$
* This means the model dynamically chooses **which document** to use for each token.
* More flexible: can pull from different documents while generating.

### üü¢ **Advantage**:

* Better for **long, complex, multi-fact outputs**, like Jeopardy-style questions or abstractive summaries.
* Can mix knowledge from multiple sources within a single response.

---

## üîÅ Comparison Summary

| Feature        | RAG-Sequence               | RAG-Token                            |
| -------------- | -------------------------- | ------------------------------------ |
| Document usage | Same doc for entire output | Different doc per token              |
| Flexibility    | Moderate                   | High                                 |
| Efficiency     | Faster decoding            | Slower, more computation             |
| Use case       | QA, short generation       | Complex NLG, multi-source generation |

---

## ‚úÖ In Practice

* **RAG-Token** shows better performance on **Jeopardy question generation**, due to its flexibility.
* **RAG-Sequence** performs well on tasks like **MS-MARCO QA** with less decoding overhead.

Both models are trained **end-to-end**, with **latent document marginalization**, and show strong results across tasks.
