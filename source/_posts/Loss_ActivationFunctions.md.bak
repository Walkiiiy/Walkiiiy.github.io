---
title: Loss Functions and Activation Functions(buiilding.....)
date: 2025-05-14 12:27:14
tags:
---
## 损失函数
### 一、分类任务损失函数
适用于预测类别标签的问题（如二分类、多分类）。

#### 1. 交叉熵损失（Cross-Entropy Loss）
**公式**：  
$$ L = -\frac{1}{N}\sum_{i=1}^N \sum_{c=1}^M y_{ic} \log \hat{y}_{ic} $$  
- $N$：样本数，$M$：类别数  
- $y_{ic}$：真实标签（one-hot编码，类别$c$为1，否则为0）  
- $\hat{y}_{ic}$：模型预测类别$c$的概率  

**特点**：  
- 直接衡量两个概率分布的差异，数值越小表示预测越接近真实分布。  
- 常用于多分类问题，配合Softmax激活函数使用。  

**变种**：  
- **二分类交叉熵（Binary Cross-Entropy）**：  
  当$M=2$时，公式简化为：  
  $$ L = -\frac{1}{N}\sum_{i=1}^N \left[ y_i \log \hat{y}_i + (1-y_i) \log (1-\hat{y}_i) \right] $$  
  适用于二分类任务（如正负样本区分），常搭配Sigmoid激活函数。

#### 2. 铰链损失（Hinge Loss）
**公式**：  
$$ L = \frac{1}{N}\sum_{i=1}^N \max(0, 1 - y_i \cdot f(x_i)) $$  
- $y_i \in \{-1, 1\}$：二分类真实标签（支持向量机常用标签形式）  
- $f(x_i)$：模型预测的得分（非概率值）  

**特点**：  
- 仅惩罚“错分”或“分类置信度不足”的样本（当$y_i \cdot f(x_i) \geq 1$时，损失为0）。  
- 常用于支持向量机（SVM），鼓励样本间有更大的分类间隔。  

#### 3. 焦点损失（Focal Loss）
**公式**：  
$$ L = -\frac{1}{N}\sum_{i=1}^N \alpha_{y_i} (1-\hat{y}_{y_i})^\gamma \log \hat{y}_{y_i} $$  
- $\alpha_{y_i}$：类别权重（平衡正负样本或类别不平衡问题）  
- $\gamma$：聚焦参数（$\gamma > 0$时，降低易分类样本的权重，专注于难样本）  

**特点**：  
- 专门解决类别不平衡问题，通过调节$\gamma$减少简单样本对损失的贡献。  
- 常用于目标检测、医学图像等正负样本极不均衡的场景。

#### 4. KL散度（Kullback-Leibler Divergence）
**公式**：  
$$ D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)} $$  
- $P$：真实分布，$Q$：预测分布  

**特点**：  
- 衡量两个概率分布的差异（非对称性），交叉熵可视为$-\sum P(i) \log Q(i)$，即$KL$散度减去$H(P)$（真实分布的熵）。  
- 常用于蒸馏学习（Knowledge Distillation），让学生模型模仿教师模型的输出分布。


### 二、回归任务损失函数
适用于预测连续数值的问题（如房价预测、温度预测）。

#### 1. 均方误差（Mean Squared Error, MSE）
**公式**：  
$$ L = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 $$  
- $y_i$：真实值，$\hat{y}_i$：预测值  

**特点**：  
- 计算简单，导数光滑（便于梯度下降优化），但对异常值敏感（因平方项放大误差）。  
- 广泛应用于线性回归、神经网络等回归任务。

#### 2. 平均绝对误差（Mean Absolute Error, MAE）
**公式**：  
$$ L = \frac{1}{N}\sum_{i=1}^N |y_i - \hat{y}_i| $$  

**特点**：  
- 直接衡量绝对误差，对异常值鲁棒性更强（误差线性增长，而非平方增长）。  
- 但导数在0点不可导，可能影响优化效率（可通过平滑处理解决，如Huber损失）。

#### 3. Huber损失（Huber Loss）
**公式**：  
$$ 
L = \begin{cases} 
\frac{1}{2}(y_i - \hat{y}_i)^2, & \text{当 } |y_i - \hat{y}_i| \leq \delta \\
\delta|y_i - \hat{y}_i| - \frac{1}{2}\delta^2, & \text{否则}
\end{cases}
$$  
- $\delta$：超参数，区分MSE和MAE的阈值  

**特点**：  
- 结合MSE和MAE的优点：  
  - 当误差较小时（$<\delta$），使用MSE，梯度稳定；  
  - 当误差较大时（$>\delta$），使用MAE，抑制异常值影响。  
- 常用于需要平衡异常值鲁棒性和优化效率的场景。

#### 4. 均方根误差（Root Mean Squared Error, RMSE）
**公式**：  
$$ L = \sqrt{\frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2} $$  

**特点**：  
- MSE的平方根，与真实值单位一致，便于解释。  
- 但性质与MSE相同，仍对异常值敏感。


### 三、其他常见损失函数
#### 1. 感知损失（Perceptual Loss）
**适用场景**：图像生成（如GAN、超分辨率）。  
**原理**：基于预训练神经网络（如VGG）提取的高层特征计算损失，衡量生成图像与真实图像的语义差异，而非像素级差异。  
**公式**：  
$$ L_{perc} = \frac{1}{HWC}\sum_{h,w,c} \left( \phi_{h,w,c}(x) - \phi_{h,w,c}(\hat{x}) \right)^2 $$  
- $\phi$：预训练网络的特征提取函数。  

#### 2. 对比损失（Contrastive Loss）
**适用场景**：度量学习（如人脸识别、相似性检索）。  
**公式**：  
$$ L = \frac{1}{N}\sum_{i=1}^N \left[ y_i d_i^2 + (1-y_i) \max(0, m - d_i)^2 \right] $$  
- $y_i$：样本对标签（1为同类，0为异类）  
- $d_i$：样本对的距离（如欧氏距离）  
- $m$：边界阈值  

**特点**：强制同类样本距离小于$m$，异类样本距离大于$m$，学习样本间的相似性度量。

#### 3. 三元组损失（Triplet Loss）
**适用场景**：度量学习（如FaceNet）。  
**公式**：  
$$ L = \max(0, d(a,p) - d(a,n) + m) $$  
- $a$：锚样本（Anchor），$p$：正样本（同类），$n$：负样本（异类）  
- $d(\cdot)$：距离函数  
- $m$：边界阈值  

**特点**：要求正样本距离小于负样本距离至少$m$，优化样本在特征空间的相对位置。



## 激活函数

以下是机器学习和深度学习中常见的激活函数及其特点、应用场景的详细介绍：


### 一、**线性激活函数（Linear Activation）**
#### 1. **恒等函数（Identity Function）**
- **公式**：\( f(x) = x \)
- **特点**：
  - 输出等于输入，无非线性变换。
  - 导数恒为1，梯度稳定但无法引入非线性。
- **应用场景**：
  - 回归任务的输出层（如房价预测）。
  - 仅用于需要保持线性关系的特殊场景（如早期神经网络的隐藏层，但现已极少使用）。


### 二、**早期经典激活函数（适用于浅层网络）**
#### 2. **Sigmoid函数**
- **公式**：\( f(x) = \frac{1}{1 + e^{-x}} \)
- **特点**：
  - 输出范围：(0, 1)，可用于二分类概率输出（需配合交叉熵损失）。
  - 导数：\( f'(x) = f(x)(1 - f(x)) \)，在x趋近于±∞时导数趋近于0，易导致**梯度消失**。
  - 非零中心输出（输出恒为正），可能使神经元更新方向单一（如“全正”梯度）。
- **应用场景**：
  - 二分类问题的输出层（如逻辑回归）。
  - 早期神经网络的隐藏层（现多被ReLU替代）。

#### 3. **Tanh函数（双曲正切函数）**
- **公式**：\( f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)
- **特点**：
  - 输出范围：(-1, 1)，**零中心输出**，缓解了Sigmoid的“梯度方向单一”问题。
  - 导数：\( f'(x) = 1 - (f(x))^2 \)，仍存在**梯度消失**问题（x绝对值较大时导数趋近于0）。
- **应用场景**：
  - 早期神经网络的隐藏层（如循环神经网络RNN）。
  - 生成模型（如GAN的判别器）。


### 三、**ReLU及其变体（现代神经网络主流）**
#### 4. **ReLU（Rectified Linear Unit，修正线性单元）**
- **公式**：\( f(x) = \max(0, x) \)
- **特点**：
  - **稀疏激活**：x≤0时输出为0，使部分神经元“静默”，模拟生物神经元特性，提升模型鲁棒性。
  - 计算高效（无指数运算），缓解梯度消失（x>0时导数为1，梯度稳定）。
  - **缺点**：x<0时导数为0，可能导致神经元“死亡”（如权重更新后长期输出0，无法恢复）。
- **应用场景**：
  - 几乎所有现代CNN、Transformer的隐藏层（如ResNet、BERT）。
  - 缓解梯度消失问题的首选激活函数。

#### 5. **Leaky ReLU**
- **公式**：\( f(x) = \begin{cases} 
x, & \text{if } x \geq 0 \\
\alpha x, & \text{if } x < 0 
\end{cases} \)（通常\( \alpha = 0.01 \)）
- **特点**：
  - 为负区间引入小斜率\( \alpha \)，避免神经元“死亡”。
  - 超参数\( \alpha \)可固定或自适应（如PReLU，参数由数据学习）。
- **应用场景**：
  - 图像生成模型（如GAN）、医学图像分析等需要避免神经元死亡的场景。

#### 6. **ELU（Exponential Linear Unit）**
- **公式**：\( f(x) = \begin{cases} 
x, & \text{if } x \geq 0 \\
\alpha (e^x - 1), & \text{if } x < 0 
\end{cases} \)（\( \alpha \)为超参数，通常取0.1-1）
- **特点**：
  - 负区间采用指数形式，输出更接近零均值，加速收敛。
  - 计算稍复杂（含指数运算），但缓解了ReLU的“死亡”问题。
- **应用场景**：
  - 对收敛速度要求高的场景（如ResNet变种）。


### 四、**自适应激活函数（带参数学习）**
#### 7. **Swish**
- **公式**：\( f(x) = x \cdot \sigma(\beta x) \)（\( \sigma \)为Sigmoid，\( \beta \)可固定或学习）
- **特点**：
  - 平滑非线性，兼具ReLU的高效和Sigmoid的饱和特性。
  - 导数始终为正，避免神经元死亡，且在x→-∞时渐近于0（类似ELU）。
- **应用场景**：
  - Google提出的EfficientNet、MobileNetV3等轻量级模型。

#### 8. **Mish**
- **公式**：\( f(x) = x \cdot \tanh(\ln(1 + e^x)) \)
- **特点**：
  - 平滑无上界，输出范围近似(-∞, +∞)，理论上比ReLU更接近生物神经元响应。
  - 计算复杂度较高，但在某些计算机视觉任务中表现优于ReLU。
- **应用场景**：
  - 目标检测模型（如YOLOv4）。


### 五、**其他特殊场景激活函数**
#### 9. **Softmax**
- **公式**：\( f(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}} \)
- **特点**：
  - 多分类任务专用，将输出转化为概率分布（总和为1）。
  - 常与交叉熵损失联合使用，本质上是归一化的指数函数。
- **应用场景**：
  - 图像分类（如MNIST的10分类）、文本多分类的输出层。

#### 10. **硬双曲正切函数（Hardtanh）**
- **公式**：\( f(x) = \max(-1, \min(1, x)) \)
- **特点**：
  - 强制将输出截断到[-1, 1]，用分段线性近似tanh，计算更高效。
- **应用场景**：
  - 强化学习的策略网络（输出动作值需限定范围）。