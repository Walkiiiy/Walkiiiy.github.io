---
title: Self-RAG
date: 2025-07-30 15:38:30
tags:
---
**their code:https://selfrag.github.io/**
**from"SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND  CRITIQUE THROUGH SELF-REFLECTION" on ICLR**
```
indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation.
```
## Overview
![self-RAG](/images/self-RAG.png)
unlike conventional RAG, self-RAG predics a "Retrieve" token on every generating round, only "Retreive"=yes will retreiver start fetching relevant documents. After using each of the fetched document for generating next token candidate, "is reall", "is supported", "is useful" will be checked using the candidates, documents and input question.
![self-RAG_processure](/images/self-RAG_processure.png)
## Reflection Tokens
![self_rag_specialTokens](/images/self_rag_specialTokens.png)
## Critic data collection
refine original open-domain QA datasets, using strong LLM like GPT-4 giving prompt(with few shot) like:
![self-RAG_dataCollectingInstruction](/images/self-RAG_dataCollectingInstruction.png)
![self-RAG_dataCollectingOutput](/images/self-RAG_dataCollectingOutput.png)
## Critic Training
- fine-tune Critic model using the refined Critic dataset QA pairs:
    $$
     \text{max}_{C} \mathbb{E}_{(x, y, r) \sim D_{\text{critic}}} \log p_C(r | x, y)
    $$
- note that the Critic only responsble for generating the Refelction Tokens. The input $x$ and $y$ are the original query and answer.
## Generator data collection
- using the former critic model C to process more QA-set pairs
## Generator Traing
- fine-tune generator model using the refined Generator dataset QA pairs(generated by critic model $c$):
  $$
  \text{max}_{M} \mathbb{E}_{(x, y, r) \sim D_{\text{gen}}} \log p_M(y, r | x)
  $$
- note that Generator not only generates a output $y$ but also a refelction token $r$
## details
- threshold: an adaptive retrieval threshold is set by considering the output probability of $retreival$ token and other token.
- Tree-decoding with critique tokens(select one from mutiple retrieved documents):
  - when k documents were fetched, their scores were calculated using scoring algorithm:
   $$
   f(y_t, d, \text{Critique}) = p(y_t | x, d, y<t) + S(\text{Critique})
   $$

   where $S(\text{Critique})$ is the total score from the critique tokens, calculated as:

   $$
   S(\text{Critique}) = \sum_{G \in G_{\text{critique}}} w_G s_G(t)
   $$
   - $p$ is the probability of segment $y$ being genertaed
   - $w$ is super paramater used to dynamically adjust the weight of critique tokens:$ISREL , ISSUP , ISUSE$
   - $s_G(t)$ is the probability of the good side of the token. 
   - For examle,  $ISREL=relevant$ is 0.05, $ISREL=irrelevant$ is 0.95, then $s_G(t)=0.05$
   - select the top-score output segment.
## Expriments
    
```
As a default configuration, we assign the weight terms ISREL , ISSUP , ISUSE values of 1.0, 1.0 and 0.5
To encourage frequent retrieval, we set the retrieval threshold to 0.2
By default, we use the top five documents from Contriever-MS MARCO (Izacard et al., 2022a) 
for biographies and open-domain QA, we use additional top five documents retrieved by a web search engine, following Luo et al. (2023)
```

![self_RAG_res](/images/self_RAG_res.png)
